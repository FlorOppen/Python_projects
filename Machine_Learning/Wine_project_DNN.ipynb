{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iO8900xHFxQ3"
   },
   "source": [
    "# Redes densas: load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A0ESU7Wgp04B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "be0a-nPpp7US"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling1D, Conv1D\n",
    "from tensorflow.keras.metrics import MSE\n",
    "from tensorflow.keras.optimizers.legacy import Adadelta\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EUzXPzhF5h8"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YM6t0P52qmzb"
   },
   "outputs": [],
   "source": [
    "wines = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wines['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0     0.0  \n",
       "1                            3.40   1050.0     0.0  \n",
       "2                            3.17   1185.0     0.0  \n",
       "3                            3.45   1480.0     0.0  \n",
       "4                            2.93    735.0     0.0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0     2.0  \n",
       "174                          1.56    750.0     2.0  \n",
       "175                          1.56    835.0     2.0  \n",
       "176                          1.62    840.0     2.0  \n",
       "177                          1.60    560.0     2.0  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine = pd.DataFrame( np.c_[wines['data'], wines['target']], columns=np.append(wines['feature_names'], ['target']) )\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_wine, hue = 'target', vars = wines['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "lr = 1.0\n",
    "epochs = 30\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgnHoc4wB1fm"
   },
   "source": [
    "## Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dividir el dataset usando train_test_split()\n",
    "* Pasar las etiquetas a vectores en formato *one-hot encoding* con la funcion _to_categorical()_\n",
    "* Crear un modelo de **redes densas** y resolver el problema. Graficar las curvas de Accuracy y MSE (u otra metrica a eleccion) en funcion de las _epochs_.\n",
    "* Â¿Como cambia el resultado si aplico esta funcion de pre-procesamiento antes del entrenamiento? **IMPORTANTE:** Aplicar sobre los datos de *train* y *test*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler.fit(wines.data)\n",
    "#datos = scaler.fit_transform(wines.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear x_train, x_test, y_train, y_test\n",
    "# 1 - fitear el scaler con x_train.\n",
    "# 2 - transformar x_train con el scaler.\n",
    "# 3 - transformar x_test con el scaler (sin volver a fitear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wines.data, wines.target, test_size=0.25, random_state=42)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes) #one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape = (X_train.shape[1:]))\n",
    "dense_layer_1 = Dense(128, activation = 'relu')(input_layer)\n",
    "dense_layer_2 = Dense(64, activation = 'relu')(dense_layer_1)\n",
    "dropout =  Dropout(0.2)(dense_layer_2)\n",
    "dense_layer_3 = Dense(32, activation = 'relu')(dropout)\n",
    "dense_layer_4 = Dense(16, activation = 'relu')(dense_layer_3)\n",
    "dropout_2 = Dropout(0.2)(dense_layer_4)\n",
    "output_layer = Dense(n_classes, activation = 'softmax')(dropout_2)\n",
    "model_dense = Model(input_layer, output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1792      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12707 (49.64 KB)\n",
      "Trainable params: 12707 (49.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Adadelta_optimizer = Adadelta(learning_rate=lr, rho=0.95)\n",
    "model_dense.compile(optimizer=Adadelta_optimizer, loss='categorical_crossentropy', metrics=['acc', 'mse'])\n",
    "model_dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 227ms/step - loss: 1.1426 - acc: 0.3083 - mse: 0.2312 - val_loss: 1.0745 - val_acc: 0.4667 - val_mse: 0.2169\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.0865 - acc: 0.3459 - mse: 0.2199 - val_loss: 1.0269 - val_acc: 0.6222 - val_mse: 0.2068\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.0394 - acc: 0.4812 - mse: 0.2100 - val_loss: 0.9842 - val_acc: 0.7333 - val_mse: 0.1972\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.9967 - acc: 0.6015 - mse: 0.2006 - val_loss: 0.9478 - val_acc: 0.7556 - val_mse: 0.1893\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.9550 - acc: 0.6165 - mse: 0.1915 - val_loss: 0.9031 - val_acc: 0.7556 - val_mse: 0.1793\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9249 - acc: 0.6165 - mse: 0.1852 - val_loss: 0.8563 - val_acc: 0.8444 - val_mse: 0.1687\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8529 - acc: 0.7744 - mse: 0.1685 - val_loss: 0.8028 - val_acc: 0.8667 - val_mse: 0.1561\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8100 - acc: 0.7895 - mse: 0.1586 - val_loss: 0.7523 - val_acc: 0.8889 - val_mse: 0.1441\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7597 - acc: 0.7820 - mse: 0.1462 - val_loss: 0.7024 - val_acc: 0.9333 - val_mse: 0.1322\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7338 - acc: 0.8195 - mse: 0.1416 - val_loss: 0.6543 - val_acc: 0.9333 - val_mse: 0.1206\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6857 - acc: 0.8797 - mse: 0.1303 - val_loss: 0.5994 - val_acc: 0.9333 - val_mse: 0.1080\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.6199 - acc: 0.9173 - mse: 0.1141 - val_loss: 0.5412 - val_acc: 0.9333 - val_mse: 0.0949\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5621 - acc: 0.8722 - mse: 0.1015 - val_loss: 0.4860 - val_acc: 0.9778 - val_mse: 0.0827\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5224 - acc: 0.8947 - mse: 0.0934 - val_loss: 0.4251 - val_acc: 1.0000 - val_mse: 0.0686\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4739 - acc: 0.9023 - mse: 0.0813 - val_loss: 0.3771 - val_acc: 1.0000 - val_mse: 0.0591\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4179 - acc: 0.9323 - mse: 0.0705 - val_loss: 0.3136 - val_acc: 1.0000 - val_mse: 0.0443\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3550 - acc: 0.9549 - mse: 0.0571 - val_loss: 0.2650 - val_acc: 1.0000 - val_mse: 0.0352\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2724 - acc: 0.9699 - mse: 0.0397 - val_loss: 0.2311 - val_acc: 1.0000 - val_mse: 0.0296\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2817 - acc: 0.9549 - mse: 0.0442 - val_loss: 0.2002 - val_acc: 1.0000 - val_mse: 0.0247\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2410 - acc: 0.9774 - mse: 0.0338 - val_loss: 0.1710 - val_acc: 0.9778 - val_mse: 0.0208\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2396 - acc: 0.9474 - mse: 0.0369 - val_loss: 0.1561 - val_acc: 0.9778 - val_mse: 0.0196\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2015 - acc: 0.9699 - mse: 0.0302 - val_loss: 0.1407 - val_acc: 0.9778 - val_mse: 0.0171\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1706 - acc: 0.9699 - mse: 0.0243 - val_loss: 0.1260 - val_acc: 1.0000 - val_mse: 0.0149\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2032 - acc: 0.9549 - mse: 0.0313 - val_loss: 0.1135 - val_acc: 1.0000 - val_mse: 0.0128\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1436 - acc: 0.9774 - mse: 0.0195 - val_loss: 0.1215 - val_acc: 0.9778 - val_mse: 0.0169\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1685 - acc: 0.9549 - mse: 0.0249 - val_loss: 0.1081 - val_acc: 0.9556 - val_mse: 0.0147\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1225 - acc: 0.9850 - mse: 0.0163 - val_loss: 0.0994 - val_acc: 0.9778 - val_mse: 0.0138\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1555 - acc: 0.9624 - mse: 0.0239 - val_loss: 0.0871 - val_acc: 0.9778 - val_mse: 0.0112\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0880 - acc: 0.9925 - mse: 0.0108 - val_loss: 0.1107 - val_acc: 0.9556 - val_mse: 0.0179\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1182 - acc: 0.9624 - mse: 0.0179 - val_loss: 0.0973 - val_acc: 0.9778 - val_mse: 0.0150\n",
      "\n",
      "Elapsed Dense Model training time: 1.94513 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history_dense = model_dense.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), shuffle=True, verbose=1)\n",
    "end_time = time.time()\n",
    "print('\\nElapsed Dense Model training time: {:.5f} seconds'.format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **EXTRA:** Resolver el problema anterior usando algunas capas ocultas convolucionales (Conv1D). Medir tiempos y comaparar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wines.data, wines.target, test_size=0.25, random_state=42)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes) #one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "#---------------------------------------------------------------------#\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "conv_1 = Conv1D(32, 3, activation='relu') (input_layer)\n",
    "conv_2 = Conv1D(64, 3, activation='relu') (conv_1)\n",
    "pool_1 = MaxPooling1D(pool_size=2) (conv_2)\n",
    "dropout_1 = Dropout(0.25) (pool_1)\n",
    "flatten_1 = Flatten() (dropout_1)\n",
    "dense_1 = Dense(100, activation='relu') (flatten_1)\n",
    "dropout_2 = Dropout(0.25) (dense_1)\n",
    "output_layer = Dense(n_classes, activation='softmax') (dropout_2)\n",
    "#---------------------------------------------------------------------#\n",
    "model_conv = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 13, 1)]           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            128       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 9, 64)             6208      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 4, 64)             0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 64)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               25700     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32339 (126.32 KB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 32339 (126.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_conv.compile(optimizer=Adadelta_optimizer, loss='categorical_crossentropy', metrics=['acc', 'mse'])\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 [==============================] - 1s 209ms/step - loss: 1.1190 - acc: 0.3083 - mse: 0.2264 - val_loss: 1.0365 - val_acc: 0.5333 - val_mse: 0.2085\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.0545 - acc: 0.4286 - mse: 0.2125 - val_loss: 0.9797 - val_acc: 0.6222 - val_mse: 0.1950\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 1.0137 - acc: 0.4737 - mse: 0.2028 - val_loss: 0.9119 - val_acc: 0.8444 - val_mse: 0.1807\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.9255 - acc: 0.6692 - mse: 0.1842 - val_loss: 0.8608 - val_acc: 0.8000 - val_mse: 0.1680\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8909 - acc: 0.6316 - mse: 0.1759 - val_loss: 0.8034 - val_acc: 0.8667 - val_mse: 0.1548\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8180 - acc: 0.7669 - mse: 0.1587 - val_loss: 0.7564 - val_acc: 0.8000 - val_mse: 0.1480\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7596 - acc: 0.7669 - mse: 0.1494 - val_loss: 0.7004 - val_acc: 0.8667 - val_mse: 0.1348\n",
      "Epoch 8/30\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.7161 - acc: 0.8271 - mse: 0.1394 - val_loss: 0.6401 - val_acc: 0.9111 - val_mse: 0.1188\n",
      "Epoch 9/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6841 - acc: 0.8195 - mse: 0.1302 - val_loss: 0.5806 - val_acc: 0.8889 - val_mse: 0.1062\n",
      "Epoch 10/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6422 - acc: 0.8195 - mse: 0.1227 - val_loss: 0.5408 - val_acc: 0.9333 - val_mse: 0.0985\n",
      "Epoch 11/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5715 - acc: 0.8647 - mse: 0.1086 - val_loss: 0.4817 - val_acc: 0.9111 - val_mse: 0.0850\n",
      "Epoch 12/30\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4841 - acc: 0.8872 - mse: 0.0876 - val_loss: 0.4452 - val_acc: 0.9333 - val_mse: 0.0773\n",
      "Epoch 13/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5248 - acc: 0.8271 - mse: 0.1007 - val_loss: 0.4271 - val_acc: 0.9111 - val_mse: 0.0751\n",
      "Epoch 14/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4615 - acc: 0.8421 - mse: 0.0873 - val_loss: 0.4306 - val_acc: 0.9111 - val_mse: 0.0764\n",
      "Epoch 15/30\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4805 - acc: 0.8045 - mse: 0.0943 - val_loss: 0.3613 - val_acc: 0.9333 - val_mse: 0.0620\n",
      "Epoch 16/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3705 - acc: 0.8947 - mse: 0.0654 - val_loss: 0.3152 - val_acc: 0.8889 - val_mse: 0.0521\n",
      "Epoch 17/30\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3471 - acc: 0.9098 - mse: 0.0614 - val_loss: 0.3229 - val_acc: 0.9111 - val_mse: 0.0563\n",
      "Epoch 18/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3790 - acc: 0.8496 - mse: 0.0716 - val_loss: 0.2862 - val_acc: 0.8889 - val_mse: 0.0490\n",
      "Epoch 19/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3265 - acc: 0.9098 - mse: 0.0582 - val_loss: 0.2963 - val_acc: 0.9333 - val_mse: 0.0497\n",
      "Epoch 20/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3460 - acc: 0.8872 - mse: 0.0631 - val_loss: 0.2491 - val_acc: 0.9111 - val_mse: 0.0419\n",
      "Epoch 21/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3059 - acc: 0.9248 - mse: 0.0542 - val_loss: 0.2866 - val_acc: 0.9333 - val_mse: 0.0486\n",
      "Epoch 22/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3294 - acc: 0.9023 - mse: 0.0592 - val_loss: 0.2733 - val_acc: 0.9333 - val_mse: 0.0469\n",
      "Epoch 23/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3068 - acc: 0.8872 - mse: 0.0576 - val_loss: 0.2443 - val_acc: 0.9111 - val_mse: 0.0441\n",
      "Epoch 24/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2777 - acc: 0.8947 - mse: 0.0512 - val_loss: 0.2341 - val_acc: 0.9111 - val_mse: 0.0427\n",
      "Epoch 25/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2427 - acc: 0.9173 - mse: 0.0428 - val_loss: 0.2076 - val_acc: 0.9111 - val_mse: 0.0371\n",
      "Epoch 26/30\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.2546 - acc: 0.9248 - mse: 0.0458 - val_loss: 0.2026 - val_acc: 0.9333 - val_mse: 0.0365\n",
      "Epoch 27/30\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2279 - acc: 0.9474 - mse: 0.0401 - val_loss: 0.2395 - val_acc: 0.8667 - val_mse: 0.0467\n",
      "Epoch 28/30\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2507 - acc: 0.9248 - mse: 0.0460 - val_loss: 0.1917 - val_acc: 0.9111 - val_mse: 0.0352\n",
      "Epoch 29/30\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2147 - acc: 0.9248 - mse: 0.0385 - val_loss: 0.1975 - val_acc: 0.9333 - val_mse: 0.0362\n",
      "Epoch 30/30\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2248 - acc: 0.8947 - mse: 0.0415 - val_loss: 0.1808 - val_acc: 0.9333 - val_mse: 0.0339\n",
      "\n",
      "Elapsed Dense Model training time: 1.93406 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history_conv = model_conv.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), shuffle=True, verbose=1)\n",
    "end_time = time.time()\n",
    "print('\\nElapsed Dense Model training time: {:.5f} seconds'.format(end_time-start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "redes-neuronales-densas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
